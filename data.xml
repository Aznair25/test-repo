<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Allm%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:llm&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/d+ujZqtQwjsUM7RX5YZLjq26+lI</id>
  <updated>2025-03-11T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">27999</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2412.18022v1</id>
    <updated>2024-12-23T22:34:40Z</updated>
    <published>2024-12-23T22:34:40Z</published>
    <title>Trustworthy and Efficient LLMs Meet Databases</title>
    <summary>  In the rapidly evolving AI era with large language models (LLMs) at the core,
making LLMs more trustworthy and efficient, especially in output generation
(inference), has gained significant attention. This is to reduce plausible but
faulty LLM outputs (a.k.a hallucinations) and meet the highly increased
inference demands. This tutorial explores such efforts and makes them
transparent to the database community. Understanding these efforts is essential
in harnessing LLMs in database tasks and adapting database techniques to LLMs.
Furthermore, we delve into the synergy between LLMs and databases, highlighting
new opportunities and challenges in their intersection. This tutorial aims to
share with database researchers and practitioners essential concepts and
strategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining
in the intersection between LLMs and databases.
</summary>
    <author>
      <name>Kyoungmin Kim</name>
    </author>
    <author>
      <name>Anastasia Ailamaki</name>
    </author>
    <link href="http://arxiv.org/abs/2412.18022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.18022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.10300v1</id>
    <updated>2024-06-13T21:32:56Z</updated>
    <published>2024-06-13T21:32:56Z</published>
    <title>Large Language Models as Software Components: A Taxonomy for
  LLM-Integrated Applications</title>
    <summary>  Large Language Models (LLMs) have become widely adopted recently. Research
explores their use both as autonomous agents and as tools for software
engineering. LLM-integrated applications, on the other hand, are software
systems that leverage an LLM to perform tasks that would otherwise be
impossible or require significant coding effort. While LLM-integrated
application engineering is emerging as new discipline, its terminology,
concepts and methods need to be established. This study provides a taxonomy for
LLM-integrated applications, offering a framework for analyzing and describing
these systems. It also demonstrates various ways to utilize LLMs in
applications, as well as options for implementing such integrations.
  Following established methods, we analyze a sample of recent LLM-integrated
applications to identify relevant dimensions. We evaluate the taxonomy by
applying it to additional cases. This review shows that applications integrate
LLMs in numerous ways for various purposes. Frequently, they comprise multiple
LLM integrations, which we term ``LLM components''. To gain a clear
understanding of an application's architecture, we examine each LLM component
separately. We identify thirteen dimensions along which to characterize an LLM
component, including the LLM skills leveraged, the format of the output, and
more. LLM-integrated applications are described as combinations of their LLM
components. We suggest a concise representation using feature vectors for
visualization.
  The taxonomy is effective for describing LLM-integrated applications. It can
contribute to theory building in the nascent field of LLM-integrated
application engineering and aid in developing such systems. Researchers and
practitioners explore numerous creative ways to leverage LLMs in applications.
Though challenges persist, integrating LLMs may revolutionize the way software
systems are built.
</summary>
    <author>
      <name>Irene Weber</name>
    </author>
    <link href="http://arxiv.org/abs/2406.10300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.10300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A.1; I.2.7; D.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.19888v1</id>
    <updated>2024-05-30T09:46:36Z</updated>
    <published>2024-05-30T09:46:36Z</published>
    <title>Parrot: Efficient Serving of LLM-based Applications with Semantic
  Variable</title>
    <summary>  The rise of large language models (LLMs) has enabled LLM-based applications
(a.k.a. AI agents or co-pilots), a new software paradigm that combines the
strength of LLM and conventional software. Diverse LLM applications from
different tenants could design complex workflows using multiple LLM requests to
accomplish one task. However, they have to use the over-simplified
request-level API provided by today's public LLM services, losing essential
application-level information. Public LLM services have to blindly optimize
individual LLM requests, leading to sub-optimal end-to-end performance of LLM
applications.
  This paper introduces Parrot, an LLM service system that focuses on the
end-to-end experience of LLM-based applications. Parrot proposes Semantic
Variable, a unified abstraction to expose application-level knowledge to public
LLM services. A Semantic Variable annotates an input/output variable in the
prompt of a request, and creates the data pipeline when connecting multiple LLM
requests, providing a natural way to program LLM applications. Exposing
Semantic Variables to the public LLM service allows it to perform conventional
data flow analysis to uncover the correlation across multiple LLM requests.
This correlation opens a brand-new optimization space for the end-to-end
performance of LLM-based applications. Extensive evaluations demonstrate that
Parrot can achieve up to an order-of-magnitude improvement for popular and
practical use cases of LLM applications.
</summary>
    <author>
      <name>Chaofan Lin</name>
    </author>
    <author>
      <name>Zhenhua Han</name>
    </author>
    <author>
      <name>Chengruidong Zhang</name>
    </author>
    <author>
      <name>Yuqing Yang</name>
    </author>
    <author>
      <name>Fan Yang</name>
    </author>
    <author>
      <name>Chen Chen</name>
    </author>
    <author>
      <name>Lili Qiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear on USENIX OSDI 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.19888v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.19888v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.15764v1</id>
    <updated>2024-11-24T09:24:04Z</updated>
    <published>2024-11-24T09:24:04Z</published>
    <title>LLM Online Spatial-temporal Signal Reconstruction Under Noise</title>
    <summary>  This work introduces the LLM Online Spatial-temporal Reconstruction (LLM-OSR)
framework, which integrates Graph Signal Processing (GSP) and Large Language
Models (LLMs) for online spatial-temporal signal reconstruction. The LLM-OSR
utilizes a GSP-based spatial-temporal signal handler to enhance graph signals
and employs LLMs to predict missing values based on spatiotemporal patterns.
The performance of LLM-OSR is evaluated on traffic and meteorological datasets
under varying Gaussian noise levels. Experimental results demonstrate that
utilizing GPT-4-o mini within the LLM-OSR is accurate and robust under Gaussian
noise conditions. The limitations are discussed along with future research
insights, emphasizing the potential of combining GSP techniques with LLMs for
solving spatiotemporal prediction tasks.
</summary>
    <author>
      <name>Yi Yan</name>
    </author>
    <author>
      <name>Dayu Qin</name>
    </author>
    <author>
      <name>Ercan Engin Kuruoglu</name>
    </author>
    <link href="http://arxiv.org/abs/2411.15764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.15764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2501.08579v1</id>
    <updated>2025-01-15T04:59:49Z</updated>
    <published>2025-01-15T04:59:49Z</published>
    <title>What Limits LLM-based Human Simulation: LLMs or Our Design?</title>
    <summary>  We argue that advancing LLM-based human simulation requires addressing both
LLM's inherent limitations and simulation framework design challenges. Recent
studies have revealed significant gaps between LLM-based human simulations and
real-world observations, highlighting these dual challenges. To address these
gaps, we present a comprehensive analysis of LLM limitations and our design
issues, proposing targeted solutions for both aspects. Furthermore, we explore
future directions that address both challenges simultaneously, particularly in
data collection, LLM generation, and evaluation. To support further research in
this field, we provide a curated collection of LLM-based human simulation
resources.\footnote{https://github.com/Persdre/llm-human-simulation}
</summary>
    <author>
      <name>Qian Wang</name>
    </author>
    <author>
      <name>Jiaying Wu</name>
    </author>
    <author>
      <name>Zhenheng Tang</name>
    </author>
    <author>
      <name>Bingqiao Luo</name>
    </author>
    <author>
      <name>Nuo Chen</name>
    </author>
    <author>
      <name>Wei Chen</name>
    </author>
    <author>
      <name>Bingsheng He</name>
    </author>
    <link href="http://arxiv.org/abs/2501.08579v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2501.08579v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
