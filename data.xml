<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <link href="http://arxiv.org/api/query?search_query%3Dall%3Allm%26id_list%3D%26start%3D0%26max_results%3D5" rel="self" type="application/atom+xml"/>
  <title type="html">ArXiv Query: search_query=all:llm&amp;id_list=&amp;start=0&amp;max_results=5</title>
  <id>http://arxiv.org/api/d+ujZqtQwjsUM7RX5YZLjq26+lI</id>
  <updated>2025-03-12T00:00:00-04:00</updated>
  <opensearch:totalResults xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">28105</opensearch:totalResults>
  <opensearch:startIndex xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">0</opensearch:startIndex>
  <opensearch:itemsPerPage xmlns:opensearch="http://a9.com/-/spec/opensearch/1.1/">5</opensearch:itemsPerPage>
  <entry>
    <id>http://arxiv.org/abs/2412.18022v1</id>
    <updated>2024-12-23T22:34:40Z</updated>
    <published>2024-12-23T22:34:40Z</published>
    <title>Trustworthy and Efficient LLMs Meet Databases</title>
    <summary>  In the rapidly evolving AI era with large language models (LLMs) at the core,
making LLMs more trustworthy and efficient, especially in output generation
(inference), has gained significant attention. This is to reduce plausible but
faulty LLM outputs (a.k.a hallucinations) and meet the highly increased
inference demands. This tutorial explores such efforts and makes them
transparent to the database community. Understanding these efforts is essential
in harnessing LLMs in database tasks and adapting database techniques to LLMs.
Furthermore, we delve into the synergy between LLMs and databases, highlighting
new opportunities and challenges in their intersection. This tutorial aims to
share with database researchers and practitioners essential concepts and
strategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining
in the intersection between LLMs and databases.
</summary>
    <author>
      <name>Kyoungmin Kim</name>
    </author>
    <author>
      <name>Anastasia Ailamaki</name>
    </author>
    <link href="http://arxiv.org/abs/2412.18022v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2412.18022v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.DB" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2406.10300v1</id>
    <updated>2024-06-13T21:32:56Z</updated>
    <published>2024-06-13T21:32:56Z</published>
    <title>Large Language Models as Software Components: A Taxonomy for
  LLM-Integrated Applications</title>
    <summary>  Large Language Models (LLMs) have become widely adopted recently. Research
explores their use both as autonomous agents and as tools for software
engineering. LLM-integrated applications, on the other hand, are software
systems that leverage an LLM to perform tasks that would otherwise be
impossible or require significant coding effort. While LLM-integrated
application engineering is emerging as new discipline, its terminology,
concepts and methods need to be established. This study provides a taxonomy for
LLM-integrated applications, offering a framework for analyzing and describing
these systems. It also demonstrates various ways to utilize LLMs in
applications, as well as options for implementing such integrations.
  Following established methods, we analyze a sample of recent LLM-integrated
applications to identify relevant dimensions. We evaluate the taxonomy by
applying it to additional cases. This review shows that applications integrate
LLMs in numerous ways for various purposes. Frequently, they comprise multiple
LLM integrations, which we term ``LLM components''. To gain a clear
understanding of an application's architecture, we examine each LLM component
separately. We identify thirteen dimensions along which to characterize an LLM
component, including the LLM skills leveraged, the format of the output, and
more. LLM-integrated applications are described as combinations of their LLM
components. We suggest a concise representation using feature vectors for
visualization.
  The taxonomy is effective for describing LLM-integrated applications. It can
contribute to theory building in the nascent field of LLM-integrated
application engineering and aid in developing such systems. Researchers and
practitioners explore numerous creative ways to leverage LLMs in applications.
Though challenges persist, integrating LLMs may revolutionize the way software
systems are built.
</summary>
    <author>
      <name>Irene Weber</name>
    </author>
    <link href="http://arxiv.org/abs/2406.10300v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2406.10300v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.CL" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="A.1; I.2.7; D.2.11" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2405.19888v1</id>
    <updated>2024-05-30T09:46:36Z</updated>
    <published>2024-05-30T09:46:36Z</published>
    <title>Parrot: Efficient Serving of LLM-based Applications with Semantic
  Variable</title>
    <summary>  The rise of large language models (LLMs) has enabled LLM-based applications
(a.k.a. AI agents or co-pilots), a new software paradigm that combines the
strength of LLM and conventional software. Diverse LLM applications from
different tenants could design complex workflows using multiple LLM requests to
accomplish one task. However, they have to use the over-simplified
request-level API provided by today's public LLM services, losing essential
application-level information. Public LLM services have to blindly optimize
individual LLM requests, leading to sub-optimal end-to-end performance of LLM
applications.
  This paper introduces Parrot, an LLM service system that focuses on the
end-to-end experience of LLM-based applications. Parrot proposes Semantic
Variable, a unified abstraction to expose application-level knowledge to public
LLM services. A Semantic Variable annotates an input/output variable in the
prompt of a request, and creates the data pipeline when connecting multiple LLM
requests, providing a natural way to program LLM applications. Exposing
Semantic Variables to the public LLM service allows it to perform conventional
data flow analysis to uncover the correlation across multiple LLM requests.
This correlation opens a brand-new optimization space for the end-to-end
performance of LLM-based applications. Extensive evaluations demonstrate that
Parrot can achieve up to an order-of-magnitude improvement for popular and
practical use cases of LLM applications.
</summary>
    <author>
      <name>Chaofan Lin</name>
    </author>
    <author>
      <name>Zhenhua Han</name>
    </author>
    <author>
      <name>Chengruidong Zhang</name>
    </author>
    <author>
      <name>Yuqing Yang</name>
    </author>
    <author>
      <name>Fan Yang</name>
    </author>
    <author>
      <name>Chen Chen</name>
    </author>
    <author>
      <name>Lili Qiu</name>
    </author>
    <arxiv:comment xmlns:arxiv="http://arxiv.org/schemas/atom">To appear on USENIX OSDI 2024</arxiv:comment>
    <link href="http://arxiv.org/abs/2405.19888v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2405.19888v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.AI" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2311.10372v2</id>
    <updated>2024-01-08T05:41:51Z</updated>
    <published>2023-11-17T07:55:16Z</published>
    <title>A Survey of Large Language Models for Code: Evolution, Benchmarking, and
  Future Trends</title>
    <summary>  General large language models (LLMs), represented by ChatGPT, have
demonstrated significant potential in tasks such as code generation in software
engineering. This has led to the development of specialized LLMs for software
engineering, known as Code LLMs. A considerable portion of Code LLMs is derived
from general LLMs through model fine-tuning. As a result, Code LLMs are often
updated frequently and their performance can be influenced by the base LLMs.
However, there is currently a lack of systematic investigation into Code LLMs
and their performance. In this study, we conduct a comprehensive survey and
analysis of the types of Code LLMs and their differences in performance
compared to general LLMs. We aim to address three questions: (1) What LLMs are
specifically designed for software engineering tasks, and what is the
relationship between these Code LLMs? (2) Do Code LLMs really outperform
general LLMs in software engineering tasks? (3) Which LLMs are more proficient
in different software engineering tasks? To answer these questions, we first
collect relevant literature and work from five major databases and open-source
communities, resulting in 134 works for analysis. Next, we categorize the Code
LLMs based on their publishers and examine their relationships with general
LLMs and among themselves. Furthermore, we investigate the performance
differences between general LLMs and Code LLMs in various software engineering
tasks to demonstrate the impact of base models and Code LLMs. Finally, we
comprehensively maintained the performance of LLMs across multiple mainstream
benchmarks to identify the best-performing LLMs for each software engineering
task. Our research not only assists developers of Code LLMs in choosing base
models for the development of more advanced LLMs but also provides insights for
practitioners to better understand key improvement directions for Code LLMs.
</summary>
    <author>
      <name>Zibin Zheng</name>
    </author>
    <author>
      <name>Kaiwen Ning</name>
    </author>
    <author>
      <name>Yanlin Wang</name>
    </author>
    <author>
      <name>Jingwen Zhang</name>
    </author>
    <author>
      <name>Dewu Zheng</name>
    </author>
    <author>
      <name>Mingxi Ye</name>
    </author>
    <author>
      <name>Jiachi Chen</name>
    </author>
    <link href="http://arxiv.org/abs/2311.10372v2" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2311.10372v2" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.SE" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
  <entry>
    <id>http://arxiv.org/abs/2411.15764v1</id>
    <updated>2024-11-24T09:24:04Z</updated>
    <published>2024-11-24T09:24:04Z</published>
    <title>LLM Online Spatial-temporal Signal Reconstruction Under Noise</title>
    <summary>  This work introduces the LLM Online Spatial-temporal Reconstruction (LLM-OSR)
framework, which integrates Graph Signal Processing (GSP) and Large Language
Models (LLMs) for online spatial-temporal signal reconstruction. The LLM-OSR
utilizes a GSP-based spatial-temporal signal handler to enhance graph signals
and employs LLMs to predict missing values based on spatiotemporal patterns.
The performance of LLM-OSR is evaluated on traffic and meteorological datasets
under varying Gaussian noise levels. Experimental results demonstrate that
utilizing GPT-4-o mini within the LLM-OSR is accurate and robust under Gaussian
noise conditions. The limitations are discussed along with future research
insights, emphasizing the potential of combining GSP techniques with LLMs for
solving spatiotemporal prediction tasks.
</summary>
    <author>
      <name>Yi Yan</name>
    </author>
    <author>
      <name>Dayu Qin</name>
    </author>
    <author>
      <name>Ercan Engin Kuruoglu</name>
    </author>
    <link href="http://arxiv.org/abs/2411.15764v1" rel="alternate" type="text/html"/>
    <link title="pdf" href="http://arxiv.org/pdf/2411.15764v1" rel="related" type="application/pdf"/>
    <arxiv:primary_category xmlns:arxiv="http://arxiv.org/schemas/atom" term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="cs.LG" scheme="http://arxiv.org/schemas/atom"/>
    <category term="eess.SP" scheme="http://arxiv.org/schemas/atom"/>
  </entry>
</feed>
